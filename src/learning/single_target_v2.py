# Single-target v2 (4 meta-targets - Information Systems)
# GMM, TMM-GS, and TMM-S
import os
# os.chdir('../../')
import time

from sklearn.tree import DecisionTreeRegressor
import utils
import numpy as np
import pandas as pd
from itertools import product
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold

def read_data(data_path):
    df = pd.read_csv(data_path)
    df = df.sample(frac=1)
    drop_cols = df.columns[(df.columns.str.startswith('rc_')) | (df.columns.str.endswith('_median')) | (df.columns.str.startswith('lid_hist'))]

    df.drop('lid_entropy', axis=1, inplace=True)
    df.drop(drop_cols, axis=1, inplace=True)
    df.dropna(axis=0, inplace=True)

    df.drop(['fashion', 'mnist121d'], axis=0, inplace=True)
    df = df[df.index != 'uscities']
    df.nr_inst = df.nr_inst.astype(float)
    selector = VarianceThreshold(0.001)
    selector.fit(df)
    drop_cols_var = df.columns[~selector.get_support()].values
    df.drop(drop_cols_var, axis=1, inplace=True)

    df = df.astype(float)
    others = ['QueryTime', 'DistComp']

    df.loc[:, others] = df[others].apply(np.log)
    return df

def fit_model(X_train, y_train, random_state=0, model=RandomForestRegressor):
    reg = model(random_state=random_state, n_jobs=-1)
    reg.fit(X_train, y_train)
    return reg

def ensamble_predictions(models, X_test):
    predictions = np.array([m.predict(X_test) for m in models])
    return predictions.mean(axis=0)

def train_predict(X_train, y_train, n_models=5):
    total_elapsed_training = 0
    total_elapsed_inference = 0
    y_pred = None
    for RS in range(n_models):
        start = time.time()
        reg = fit_model(
            X_train,
            y_train,
            random_state=RS,
            model=RandomForestRegressor
        )
        total_elapsed_training += time.time() - start

        start = time.time()
        y_pred_current = reg.predict(X_test)
        total_elapsed_inference += time.time() - start

        if y_pred is None:
            y_pred = y_pred_current
        else:
            y_pred += y_pred_current

        del(reg)
    y_pred /= n_models
    return y_pred, total_elapsed_training, total_elapsed_inference

def format_scores(base_target, metatarget, approach, y_test, y_pred, elapsed_training, elapsed_inference):
    return pd.Series({
        "base": base_target,
        "target": metatarget,
        "approach": approach.__name__,
        "r2": r2_score(y_test, y_pred),
        "mae": mean_absolute_error(y_test, y_pred),
        "mse": mean_squared_error(y_test, y_pred),
        "elapsed_training": elapsed_training,
        "elapsed_inference": elapsed_inference
    }).to_frame().T

def format_predictions(X_test, y_test, y_pred, metatarget, approach):
    return pd.DataFrame({
        "base": X_test.index,
        "true": y_test[metatarget].values,
        "pred": y_pred,
        "target": [metatarget] * len(y_test),
        "NN": X_test.IndexParams.values,
        "R": X_test.QueryTimeParams.values,
        "graph_type": X_test.graph_type.values,
        "nr_inst": X_test.nr_inst.values,
        "approach": [approach.__name__] * len(y_test),
        # "k_searching": X_test.k_searching.values
    })

def gmm(data, base_target):
    """
        Generic meta-model (GMM).
        ----------------------------------------------
        All meta-instances of our meta-dataset regarding all datasets
        were used for meta-training, except for the meta-instances
        regarding the goal dataset, which was used for meta-testing.
        ----------------------------------------------
    """
    base, nr_inst = base_target.split('_')
    train = data[data.index != base]
    test = data[(data.index == base) & (data.nr_inst == np.log(int(nr_inst)))]
    assert not test.empty, base_target
    return train, test

def tmm_gs(data, base_target):
    """
        Tuned meta-model using grid search (TMM-GS).
        ----------------------------------------------
        All meta-instances of our meta-dataset regarding all datasets
        (except for the ones referring to the goal dataset) plus meta-
        instances generated by a grid search performed over the goal
        dataset were used for meta-training and the goal dataset was
        used for testing.
        ----------------------------------------------
        GS params:
        NN = [1, 25, 70, 150],
        R = [1, 10, 40, 120],
        k = 30
    """
    base, nr_inst = base_target.split('_')
    nr_inst = np.log(int(nr_inst))
    train = data[data.index != base]
    test = data[(data.index == base) & (data.nr_inst == nr_inst)]
    gs_instances = test[
        (test.IndexParams.isin([5,25,70, 150])) &
        (test.QueryTimeParams.isin([1,10,40,120])) # &
        # (test.nr_inst == test.nr_inst.max())
    ]
    train = pd.concat([train, gs_instances])
    return train, test

def tmm_s(data, base_target):
    """
    Tuned meta-model using subsets (TMM-S).
    ----------------------------------------------
    Meta-instances of our meta-dataset regarding all datasets
    (except for the ones regarding the goal dataset) plus meta-
    instances of subsets of the real datasets were used for meta-
    training, the remaining meta-instances were used for meta-testing.
    ----------------------------------------------
    """
    # sizes = [69900, 25274, 67940, 999900] # sizes of complete datasets
    base, nr_inst = base_target.split('_')
    nr_inst = np.log(int(nr_inst))
    train = data[data.index != base]
    test = data[(data.index == base) & (data.nr_inst == nr_inst)]

    subset_instances = data[(data.index.str.startswith(base)) & (data.nr_inst < nr_inst)]
    train = pd.concat([train, subset_instances])
    return train, test

def check_if_done(approach, base_target, k, metatarget, SCORES):
    return len(SCORES[
        (SCORES.approach == approach) &
        (SCORES.base == base_target) &
        (SCORES.k_searching == k) &
        (SCORES.target == metatarget)
    ])

if __name__ == "__main__":
    ##### setup
    # metabase = 'data/metabase/metabase_v3.csv'
    SCORES = pd.DataFrame(columns=['base', 'target', 'approach', 'r2', 'mae', 'mse', 'elapsed_training', 'elapsed_inference', 'k_searching'])
    RESULTS = pd.DataFrame()
    # SCORES = pd.read_csv('data/results/info_sys_interpolated/adbis/scores.csv')
    # RESULTS = pd.read_csv('data/results/info_sys_interpolated/adbis/predictions.csv')

    META_TARGETS = ['DistComp', 'Recall', 'QueryTime']#, 'IndexTime']
    APPROACHES = [gmm, tmm_gs, tmm_s]
    N_MODELS = 5
    DATASET_TARGET = [
        'mnist121d_16000', 'mnist121d_32000', 
        'mnist_16000', 'mnist_32000', 
        'cophir282_500000', 'cophir282_100000',
        'cophir64_500000', 'cophir64_100000', 
        'base71_100000', 'base71_500000',
    # ]
    # DATASET_TARGET = [
        'texture_16000', 'texture_32000',
        'sift_500000', 'sift_100000',
        'moments_16000', 'moments_32000',
        'fashion_16000', 'fashion_32000',
        'colorHisto_16000', 'colorHisto_32000',
    ]


    # reading meta-database
    data = utils.read_int_mb()
    data.drop('IndexTime', axis=1, inplace=True)
    data.reset_index(inplace=True)
    data.base = data.base.apply(lambda x: x.split('_')[0])
    data.set_index('base', inplace=True)
    
    prods = product(APPROACHES, DATASET_TARGET, [1, 10, 30])
    get_xy = lambda x: (x.drop(META_TARGETS, axis=1).copy(), x.loc[:, META_TARGETS].copy())
    for approach, base_target, k in prods:
        # if k != 30:
        #     continue
        
        # moduling data according to approach
        train, test = approach(data, base_target)

        # # train, test
        X_train, y_train = get_xy(train[train.k_searching == k])
        X_test, y_test = get_xy(test[test.k_searching == k])

        if len(X_test) == 0:
            print('ruim', base_target, approach.__name__, k)
            continue
        # inducing a meta-model for each meta-target
        for metatarget in META_TARGETS:
            if check_if_done(approach.__name__, base_target, k, metatarget, SCORES):
                print('already done')
                continue
            
            print('Fitting {} for base {} k={} via {}'.format(metatarget, base_target, k, approach.__name__))
            y_pred, total_elapsed_training, total_elapsed_inference = train_predict(X_train, y_train[metatarget])

            scores = format_scores(base_target, metatarget, approach, y_test[metatarget], y_pred, total_elapsed_training, total_elapsed_inference)
            scores['k_searching'] = k
            SCORES = pd.concat([SCORES, scores.copy()], ignore_index=True)
        
            SCORES.to_csv('data/results/info_sys_interpolated/adbis/scores1.csv', index=False)

            res = format_predictions(X_test, y_test, y_pred, metatarget, approach)
            res['k_searching'] = k
            RESULTS = pd.concat([RESULTS, res.copy()])
            RESULTS.to_csv('data/results/info_sys_interpolated/adbis/predictions1.csv', index=False)
            del(scores)
            del(res)
